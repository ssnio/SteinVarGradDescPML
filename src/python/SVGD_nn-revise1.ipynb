{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of neural network based on \n",
    "Qiang Liu and Dilin Wang, [*Stein Variational Gradient Descent (SVGD): A General Purpose Bayesian Inference Algorithm*](https://arxiv.org/pdf/1608.04471.pdf), NIPS, 2016.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF(torch.nn.Module):\n",
    "    def __init__(self, h=-1):\n",
    "        super(RBF, self).__init__()\n",
    "        self.h = h\n",
    "    \n",
    "    def forward(self, theta):\n",
    "        #theta shape \n",
    "        theta_i = theta[:,None,:]  #shape (N,D) -> N,1,D\n",
    "        theta_j = theta[None, :,:] #shape (N,D) -> 1,N,D\n",
    "        \n",
    "        pairwise_dists = torch.norm((theta_i - theta_j),dim=2).pow(2)  #[i-j]^2 i-j.shape = (N,N,D)\n",
    "        #print(pairwise_dists.shape) N,N\n",
    "        #pairwise_dists = torch.norm(theta[:,None,:] - theta, dim=2).pow(2)\n",
    "        h = self.h\n",
    "        if(h<0):\n",
    "            #do the meadian trick\n",
    "            #h = np.median(pairwise_dists)\n",
    "            p_dists = pairwise_dists.detach().flatten()\n",
    "            h=torch.median(p_dists)\n",
    "            #h = torch.median(p_dists)/np.log(theta.shape[0])\n",
    "            h = torch.sqrt(0.5*h/np.log(theta.shape[0]+1))\n",
    "\n",
    "        #compute the rbf kernel\n",
    "        Kxy = torch.exp(-pairwise_dists / h**2 / 2)\n",
    "        \n",
    "        return Kxy,h\n",
    "\n",
    "     \n",
    "    def updsate(self, model, theta):\n",
    "        n = theta.shape[0]\n",
    "        theta = theta.detach().requires_grad_(True)\n",
    "\n",
    "        logp = model.logp(theta)   \n",
    "        dlogp = torch.autograd.grad(logp.sum(), theta)[0] \n",
    "\n",
    "        Kxy, h = self.forward(theta)\n",
    "        \n",
    "        dxKxy = torch.autograd.grad(Kxy.sum(), theta)[0]/(h ** 2)\n",
    "\n",
    "        dxKxy = -(Kxy.mm(dlogp) + dxKxy) / n #question to the original code\n",
    "\n",
    "        return dxKxy    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svgd_nn:\n",
    "    '''\n",
    "    define a one hidden-layer neural network\n",
    "    Input:\n",
    "    batch size, iter, M, n_hiddenm a0, b0, master_stepsize\n",
    "    In the paper, they take 1 hidden layer with 50 units, \n",
    "    the results are averaged over 20 random trials. \n",
    "    Use ReLu as activa function\n",
    "    Use AdaGrad with momentum, 20 particles.\n",
    "    Mini-batch size is 100\n",
    "    '''\n",
    "    def __init__(self, X, y, batch_size, n_hidden, max_iter,  M, a0=1.0, b0=0.1):#, a0, b0, master_stepsize):\n",
    "        super(svgd_nn, self).__init__()\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.M = M\n",
    "        self.d = X.shape[1]\n",
    "        self.a0 = a0\n",
    "        self.b0 = b0\n",
    "        #self.gamma_prior = Gamma(torch.tensor(1., device=device), torch.tensor(1/0.1, device = device))\n",
    "        #self.lambda_prior = Gamma(torch.tensor(1., device=device), torch.tensor(1/0.1, device = device))\n",
    "        \n",
    "    def forward(self, x, theta):\n",
    "        ##w1: d*n_hidden; b1: n_hidden; w2 = n_hidden; b2 = 1; 2 variances\n",
    "        w1 = theta[:, 0:self.d * self.n_hidden].reshape(-1, self.d, self.n_hidden)\n",
    "        b1 = theta[:, self.d * self.n_hidden:(self.d + 1) * self.n_hidden].unsqueeze(1)\n",
    "        w2 = theta[:, (self.d + 1) * self.n_hidden:(self.d + 2) * self.n_hidden].unsqueeze(2)\n",
    "        b2 = theta[:, -3].reshape(-1, 1, 1)\n",
    "        \n",
    "        x = x.unsqueeze(0).repeat(self.M, 1,1)  #need to do forward for all particles\n",
    "        x = F.relu(torch.bmm(x,w1)+b1)\n",
    "        x = torch.bmm(x,w2)+b2\n",
    "        y = x.squeeze()\n",
    "        return y\n",
    "        \n",
    "    def logp(self, theta):\n",
    "        '''\n",
    "        p(y | W, X, \\gamma) = \\prod_i^N  N(y_i | f(x_i; W), \\gamma^{-1})\n",
    "        p(W | \\lambda) = \\prod_i N(w_i | 0, \\lambda^{-1})\n",
    "        p(\\gamma) = Gamma(\\gamma | a0, b0) \n",
    "        p(\\lambda) = Gamma(\\lambda | a0, b0)\n",
    "    \n",
    "        The posterior distribution is as follows:\n",
    "        p(W, \\gamma, \\lambda) = p(y | W, X, \\gamma) p(W | \\lambda) p(\\gamma) p(\\lambda) \n",
    "        '''\n",
    "        #prepare parameters\n",
    "        log_gamma = torch.exp(theta[:, -2])  #variance related to gamma\n",
    "        log_lambda = torch.exp(theta[:, -1])\n",
    "    \n",
    "        \n",
    "        #p(\\gamma) = Gamma(\\gamma | a0, b0) \n",
    "        p_gamma = Gamma(torch.tensor(self.a0, device=device), torch.tensor(1/self.b0, device = device))\n",
    "        p_lambda = Gamma(torch.tensor(self.a0, device=device), torch.tensor(1/self.b0, device = device))\n",
    "\n",
    "        \n",
    "        #get different distributions\n",
    "        #p(W | \\lambda) = \\prod_i N(w_i | 0, \\lambda^{-1})\n",
    "        w = theta[:, :-1]\n",
    "        #p_w = Normal(0, torch.ones_like(log_lambda) / log_lambda)\n",
    "        p_w = Normal(0, torch.sqrt(torch.ones_like(log_lambda) / log_lambda))\n",
    "\n",
    "        #p(y | W, X, \\gamma) = \\prod_i^N  N(y_i | f(x_i; W), \\gamma^{-1})\n",
    "        batch_index = random.sample([i for i in range(self.X.shape[0])], self.batch_size)#mini batch with random idx\n",
    "        X_batch, y_batch  = self.X[batch_index],self.y[batch_index]               \n",
    "        outputs = self.forward(X_batch, theta)  #apply the network,[M, batch_size]\n",
    "        \n",
    "        pgamma_repeat = log_gamma.unsqueeze(1).repeat(1, self.batch_size)    \n",
    "        y_batch_repeat = y_batch.unsqueeze(0).repeat(self.M, 1)\n",
    "        p_y = Normal(outputs, torch.sqrt(torch.ones_like(pgamma_repeat) / pgamma_repeat))\n",
    "        log_p_y = p_y.log_prob(y_batch_repeat).sum(dim=1)\n",
    "\n",
    "        log_p0 = p_w.log_prob(w.t()).sum(dim=0) + p_gamma.log_prob(log_gamma) + p_lambda.log_prob(log_lambda)\n",
    "        log_p = log_p_y * (self.X.shape[0] / self.batch_size) +  log_p0\n",
    "            # algorithm 1,equation 8\n",
    "        return log_p\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, X_test, y_test, theta):\n",
    "    ##test\n",
    "    temp = model.forward(X_test, theta)\n",
    "    y_pred = temp.mean(dim=0) #take mean over particles\n",
    "    rmse_test = torch.norm(y_pred - y_test)/math.sqrt(y_test.shape[0])\n",
    "    \n",
    "    #print('test result')\n",
    "    #print(y_pred)\n",
    "    #print(y_test)\n",
    "    print(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "tensor(28.8955)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:202: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.2331)\n",
      "tensor(21.1119)\n",
      "tensor(19.6167)\n",
      "tensor(18.9646)\n",
      "tensor(18.3925)\n",
      "tensor(17.4626)\n",
      "tensor(16.1759)\n",
      "tensor(14.8767)\n",
      "tensor(13.7404)\n",
      "tensor(12.6740)\n",
      "tensor(11.6590)\n",
      "tensor(10.8030)\n",
      "tensor(9.9825)\n",
      "tensor(9.2272)\n",
      "tensor(8.5218)\n",
      "tensor(7.9654)\n",
      "tensor(7.4496)\n",
      "tensor(6.9665)\n",
      "tensor(6.5375)\n",
      "tensor(6.1412)\n",
      "tensor(5.8263)\n",
      "tensor(5.4917)\n",
      "tensor(5.1988)\n",
      "tensor(4.9160)\n",
      "tensor(4.7155)\n",
      "tensor(4.5223)\n",
      "tensor(4.3178)\n",
      "tensor(4.1469)\n",
      "tensor(3.9744)\n",
      "tensor(3.8497)\n",
      "tensor(3.7382)\n",
      "tensor(3.5879)\n",
      "tensor(3.4809)\n",
      "tensor(3.3583)\n",
      "tensor(3.2852)\n",
      "tensor(3.2047)\n",
      "tensor(3.1144)\n",
      "tensor(3.0485)\n",
      "tensor(2.9757)\n",
      "tensor(2.8797)\n",
      "tensor(2.8255)\n",
      "tensor(2.7546)\n",
      "tensor(2.7308)\n",
      "tensor(2.6822)\n",
      "tensor(2.6515)\n",
      "tensor(2.5705)\n",
      "tensor(2.5381)\n",
      "tensor(2.5069)\n",
      "tensor(2.4609)\n",
      "tensor(2.4151)\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cpu')\n",
    "    \"load data file\"\n",
    "    data = np.loadtxt(\"boston_housing\")\n",
    "    #print(data.shape)\n",
    "    X_input = data[ :, range(data.shape[1] - 1) ] #input\n",
    "    y_input = data[ :, data.shape[1] - 1 ] #result\n",
    "    X_input = torch.from_numpy(X_input).float().to(device)\n",
    "    y_input = torch.from_numpy(y_input).float().to(device)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=1)\n",
    "    X_train_mean, X_train_std = torch.mean(X_train, dim=0), torch.std(X_train, dim=0)\n",
    "    y_train_mean, y_train_std = torch.mean(y_train, dim=0), torch.std(y_train, dim=0)\n",
    "    \n",
    "    X_train = (X_train - X_train_mean) / X_train_std\n",
    "    X_test = (X_test - X_train_mean) / X_train_std\n",
    "    y_train = (y_train - y_train_mean) / y_train_std\n",
    "    y_test = (y_test - y_train_mean) / y_train_std\n",
    "    \n",
    "    M, batch_size, n_hidden = 100, 200, 50\n",
    "    n_iter = 5000\n",
    "    a0 =1.0\n",
    "    b0 = 0.1\n",
    "    gamma = np.random.gamma(a0, b0)\n",
    "    lambdas = np.random.gamma(a0, b0)\n",
    "\n",
    "    model = svgd_nn(X_train, y_train, batch_size, n_hidden, n_iter,  M, a0, b0)\n",
    "    kernel = RBF(h=-1)\n",
    "\n",
    "    theta = torch.cat(\n",
    "        [torch.zeros([M, X_train.shape[1] * n_hidden ], device=device).normal_(0, math.sqrt(n_hidden)),\n",
    "         torch.zeros([M, n_hidden ], device=device),\n",
    "         torch.zeros([M, n_hidden ], device=device).normal_(0, math.sqrt(X_train.shape[1])),\n",
    "         torch.zeros([M, 1], device=device),\n",
    "         torch.log(0.1 * torch.ones([M, 2], device=device))], dim=1)\n",
    "\n",
    "    '''theta = torch.cat(\n",
    "        [torch.zeros([M, (X_train.shape[1] + 2) * n_hidden + 1], device=device).normal_(0, math.sqrt(n_hidden)),\n",
    "         torch.log(gamma* torch.ones([M, 2], device=device)),\n",
    "         torch.log(lambdas* torch.ones([M, 2], device=device))], dim=1)'''\n",
    "    \n",
    "    for epoch in range(n_iter):\n",
    "        optimizer = Adam([theta], lr=1e-3)\n",
    "        optimizer.zero_grad()\n",
    "        theta.grad = kernel.updsate(model, theta)\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            test(model, X_test, y_test, theta)\n",
    "    \n",
    "    test(model, X_test, y_test, theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
